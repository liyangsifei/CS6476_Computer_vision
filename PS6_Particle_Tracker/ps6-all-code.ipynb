{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Particle Filter Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Implement the particle filter and run it on the pres debate.avi clip. You should begin by attempting to track Romneyâ€™s face. Tweak the parameters including window size until you can get the tracker to follow his face faithfully (5-15 pixels) up until he turns his face significantly. Run the tracker and save the video frames 28, 84, and 144 with the visualizations overlaid.\n",
    "\n",
    "Output: The code, the 3 image frames with overlaid visualizations, and the image patch used for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bounding_box(filename):\n",
    "    file = open(filename, 'r')\n",
    "    for i in file:\n",
    "        xl, yl, w, h = i.split('  ')\n",
    "        return int(float(xl)), int(float(yl)), int(float(w)), int(float(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, sigma_d):\n",
    "    mean = np.array([0, 0])\n",
    "    sigma = np.array([[sigma_d, 0], [0, sigma_d]])\n",
    "    d = np.random.multivariate_normal(mean, sigma, 1)\n",
    "    new_X = X + d\n",
    "    return new_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurement(X, template, image, sigma_MSE):\n",
    "    win_h, win_w = template.shape\n",
    "    x = int(X[1])\n",
    "    y = int(X[0])\n",
    "    image_part = image[y - int(win_h/2) : y + int(win_h/2+0.5), x - int(win_w/2) : x + int(win_w/2+0.5)]\n",
    "    if (template.shape != image_part.shape):\n",
    "        return 0\n",
    "    diff = np.matrix(template, dtype=np.float64) - np.matrix(image_part, dtype=np.float64)\n",
    "    MSE = 1/(win_h*win_w) * np.sum(np.multiply(diff, diff))\n",
    "    p = np.exp((-1)* MSE / (2 * (sigma_MSE ** 2)))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(particles, weights, template, image_t, sigma_d, sigma_MSE):\n",
    "    new_particles = []\n",
    "    new_weights = []\n",
    "    weight_sum = 0\n",
    "    for i in range(len(particles)):\n",
    "        p_index = np.random.choice(range(len(particles)), p=weights)\n",
    "        sample_p = particles[p_index]\n",
    "    \n",
    "        # Sample from prediction\n",
    "        new_p = prediction(sample_p, sigma_d)\n",
    "        # weight by measurement\n",
    "        w = measurement(new_p, template, image_t, sigma_MSE)\n",
    "        new_weights.append(w)\n",
    "        weight_sum += w\n",
    "        new_particles.append(new_p)\n",
    "    \n",
    "    for i in range(len(particles)):\n",
    "        new_weights[i] = new_weights[i]/weight_sum\n",
    "    return new_particles, new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_filter(video_file='pres_debate.avi', txt_file='pres_debate.txt', window_scaling=1, nb_particle=10,\\\n",
    "                    sigma_d=10, sigma_MSE=10, frame_output=[28, 84, 144], id_='1-1',\\\n",
    "                    all_frames=False, video_output='pres_debate_tracking'):\n",
    "    \"\"\"\n",
    "    video_file: source video file\n",
    "    txt_file: source bounding rectangle file\n",
    "    window_scaling: for scaling different size of window\n",
    "    nb_particle: number of particles initialize\n",
    "    sigma_d: sigma in prediction model\n",
    "    sigma_MSE: sigma for MSE calculation in updating weight\n",
    "    frame_output: special frame output, list with numbers\n",
    "    id: video and images' id\n",
    "    all_frames: if True, output all the frames in a file named with id_\n",
    "    video_output: vido output file name. If False, no video output\n",
    "    \"\"\"\n",
    "    pres_debate = cv2.VideoCapture(video_file)\n",
    "    count = 0\n",
    "    \n",
    "    rect_x, rect_y, rect_w, rect_h = read_bounding_box(txt_file)\n",
    "    init_particle_center = np.array((int(rect_y + rect_h/2), int(rect_x + rect_w/2)))\n",
    "    \n",
    "    weights = np.ones((nb_particle)) / nb_particle\n",
    "\n",
    "    particles = np.random.multinomial(100, [1/2.] * 2, size=nb_particle)\n",
    "    particles = particles - 50\n",
    "    for idx in range(len(particles)):\n",
    "        particles[idx] = particles[idx] + init_particle_center\n",
    "    ret, frame = pres_debate.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    template = gray[rect_y : rect_y + rect_h, rect_x : rect_x + rect_w]\n",
    "    if window_scaling != 1:\n",
    "        template = cv2.resize(template, (0, 0), template, fx=window_scaling, fy=window_scaling)\n",
    "    win_h, win_w = template.shape\n",
    "    \n",
    "    count += 1\n",
    "    for p in particles:\n",
    "        frame = cv2.circle(frame, (int(p[1]), int(p[0])), 1, (255, 0, 0), 1)\n",
    "    if all_frames:\n",
    "        cv2.imwrite('./{}/0.png'.format(id_), frame)\n",
    "    \n",
    "    if video_output:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(\"{}-{}-p{}-mse{}-win{}.avi\".format(video_output, id_, nb_particle, sigma_MSE, window_scaling),\\\n",
    "                              fourcc, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        out.write(frame)\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while(pres_debate.isOpened()):\n",
    "        ret, frame = pres_debate.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        particles, weights = update(particles, weights, template, gray, sigma_d, sigma_MSE)\n",
    "    \n",
    "        mean_p = np.mean(particles, axis=0)\n",
    "        mean_rect_LT = [int(mean_p[1] - win_w/2), int(mean_p[0] - win_h/2)]\n",
    "        mean_rect_RB = [int(mean_p[1] + win_w/2), int(mean_p[0] + win_h/2)]\n",
    "\n",
    "        for p in particles:\n",
    "            frame = cv2.circle(frame, (int(p[1]), int(p[0])), 1, (255, 0, 0), 1)\n",
    "        frame = cv2.rectangle(frame, (mean_rect_LT[0], mean_rect_LT[1]), (mean_rect_RB[0], mean_rect_RB[1]),\\\n",
    "                              (0, 255, 0), 1)\n",
    "        if all_frames:\n",
    "            cv2.imwrite('./{}/{}.png'.format(id_, count), frame)\n",
    "        if count in frame_output:\n",
    "            cv2.imwrite('ps6-{}-{}-p{}-mse{}-win{}.png'.format(id_, count, nb_particle, sigma_MSE, window_scaling),\\\n",
    "                        frame)\n",
    "        count += 1\n",
    "        if video_output:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            out.write(frame)\n",
    "        #break\n",
    "    pres_debate.release()\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Elapsed: \", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed:  4.953596353530884\n"
     ]
    }
   ],
   "source": [
    "particle_filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Particles; sigmaMSE=10; sigma d=10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ps6-1-1-28-p10-mse10-win1.png\" width=\"500\" alt=\"particle10\" align=left />\n",
    "<img src=\"ps6-1-1-84-p10-mse10-win1.png\" width=\"500\" alt=\"particle10\" align=left />\n",
    "<img src=\"ps6-1-1-144-p10-mse10-win1.png\" width=\"500\" alt=\"particle10\" align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Experiment with different dimensions for the window image patch you are trying to track. Decrease the window size until the performance of the tracker degrades significantly. Try significantly larger windows than what worked in 1.1. Discuss the trade-offs of window size and what makes some image patches work better than others for tracking.\n",
    "\n",
    "Output: Discussion in the pdf. Indicate 2-3 advantages of larger window size and 2-3 advantages of smaller window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed:  9.193111658096313\n"
     ]
    }
   ],
   "source": [
    "particle_filter(window_scaling=3, id_='1-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "I tried to resize the template from 0.3 to 3 times. All the trackers can be tracking the face or part of the face. I think it's because, in this video, the face we are tracking has a distinctive gray-scale color with its background. So if the particle can find the right brightness pattern, it can retrieve the skin and take it as the face. \n",
    "\n",
    "Advantages of larger window size:\n",
    "1. Can keep the full face in the detection frame. \n",
    "2. When there is a change in the face, can still catch a global pattern\n",
    "\n",
    "Advantages of smaller window size:\n",
    "1. Running significantly faster than the larger window size.\n",
    "2. Lower the effect by background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Adjust the ÏƒMSE parameter to higher and lower values and run the tracker. Discuss how changing this parameter alters the results and attempt to explain why.\n",
    "\n",
    "Output: Discussion in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed:  5.097007751464844\n"
     ]
    }
   ],
   "source": [
    "particle_filter(sigma_MSE=50, id_='1-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "I tried with ÏƒMSE=3, 10, 15, 20, 50\n",
    "\n",
    "The particle filter still works well with lower ÏƒMSE values (the lowest possible I tried was 3), but the performance starts to degrade after ÏƒMSE=20. After the face changes its formation, the particle cannot find the right position; even the face is distinctive with the background. I think it's because a large ÏƒMSE makes every possibility similar. So it hardly selects the best particles into the next frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Try and optimize the number of particles needed to track the target. Discuss the trade-offs of using a larger number of particles to represent the distribution.\n",
    "\n",
    "Output: Optimized particle number and discussion in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed:  12.591731071472168\n"
     ]
    }
   ],
   "source": [
    "particle_filter(nb_particle=50, id_='1-4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "Running time:\n",
    "50 particles ~12s\n",
    "10 particles ~5s\n",
    "5 particles ~4s\n",
    "\n",
    "A larger number of particles takes more time to run. But it is more sensitive to the small motion of the face. With 50 particles, we can see the rectangular frame moves with the face motion. A small number of particles run faster but more imprecise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 Run your tracker on noisy debate.avi and see what happens. Tune your parameters so that the cluster is able to latch back onto his face after the noise disappears. Include varying ÏƒMSE. Report how the particles respond to increasing and decreasing noise. Save the video frames 14, 32, and 46 with the visualizations overlaid.\n",
    "\n",
    "Output: The code, the 3 image frames with overlaid visualizations, and discussion in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed:  14.896376848220825\n"
     ]
    }
   ],
   "source": [
    "particle_filter(video_file='noisy_debate.avi', txt_file='noisy_debate.txt', window_scaling=1, nb_particle=10,\\\n",
    "                sigma_d=10, sigma_MSE=10, id_='1-5', frame_output = [14, 32, 46],\\\n",
    "                video_output='noisy_debate_tracking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "5 particles do not work with the noisy version video, but it works with the no-noise one. Because of the noise, there might be some pattern that has the same brightness as the face. Small particles number cannot ensure a particle that finds the right face position but not the noise. 7 Particles can already track the face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Particles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ps6-1-5-14-p7-mse10-win1.png\" width=\"500\" alt=\"particle10\" align=left />\n",
    "<img src=\"ps6-1-5-32-p7-mse10-win1.png\" width=\"500\" alt=\"particle10\" align=left />\n",
    "<img src=\"ps6-1-5-46-p7-mse10-win1.png\" width=\"500\" alt=\"particle10\" align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Appearance Model Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Implement the appearance model update. Run the tracker on pres_debate.avi and adjust parameters until you can track Romneyâ€™s hand up to frame 140. Run the tracker and save the video frames 15, 50, and 140 with the visualizations overlaid.\n",
    "\n",
    "Output: The code, the 3 image frames with overlaid visualizations, and the image patch used for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_template(particles, weights, template, image_t, sigma_d, sigma_MSE):\n",
    "    new_particles = []\n",
    "    new_weights = []\n",
    "    weight_sum = 0\n",
    "    for i in range(len(particles)):\n",
    "        p_index = np.random.choice(range(len(particles)), p=weights)\n",
    "        sample_p = particles[p_index]\n",
    "    \n",
    "        # Sample from prediction\n",
    "        new_p = prediction(sample_p, sigma_d)\n",
    "        # weight by measurement\n",
    "        w = measurement(new_p, template, image_t, sigma_MSE)\n",
    "        new_weights.append(w)\n",
    "        weight_sum += w\n",
    "        new_particles.append(new_p)\n",
    "    \n",
    "    for i in range(len(particles)):\n",
    "        new_weights[i] = new_weights[i]/weight_sum\n",
    "    best_template_yx = new_particles[np.argmax(new_weights)]\n",
    "    best_template_yx[0] = int(best_template_yx[0])\n",
    "    best_template_yx[1] = int(best_template_yx[1])\n",
    "    return new_particles, new_weights, best_template_yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_filter_appearance_model(video_file='pres_debate.avi', nb_particle=10, alpha=0.5, origin_temp=False,\\\n",
    "                                     sigma_d=10, sigma_MSE=10, frame_output=[15, 50, 140],id_='2-1',\\\n",
    "                                     all_frames=False, video_output='pres_debate_hand_tracking'):\n",
    "    pres_debate = cv2.VideoCapture(video_file)\n",
    "    count = 0\n",
    "    \n",
    "    rect_x, rect_y, rect_w, rect_h = 533, 383, 74, 105\n",
    "    init_particle_center = np.array((int(rect_y + rect_h/2), int(rect_x + rect_w/2)))\n",
    "    \n",
    "    weights = np.ones((nb_particle)) / nb_particle\n",
    "\n",
    "    particles = np.random.multinomial(100, [1/2.] * 2, size=nb_particle)\n",
    "    particles = particles - 50\n",
    "    for idx in range(len(particles)):\n",
    "        particles[idx] = particles[idx] + init_particle_center\n",
    "    ret, frame = pres_debate.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    template = gray[rect_y :rect_y + rect_h, rect_x : rect_x + rect_w]\n",
    "    origin_template = template\n",
    "    win_h, win_w = template.shape\n",
    "    cv2.imwrite('ps6-{}-template.png'.format(id_), template)\n",
    "    \n",
    "    count += 1\n",
    "    for p in particles:\n",
    "        frame = cv2.circle(frame, (int(p[1]), int(p[0])), 1, (255, 0, 0), 1)\n",
    "    frame = cv2.rectangle(frame, (rect_x, rect_y), (rect_x+rect_w, rect_y+rect_h),(0, 255, 0), 1)\n",
    "    if all_frames:\n",
    "        cv2.imwrite('./{}/0.png'.format(id_), frame)\n",
    "    \n",
    "    if video_output:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(\"{}-{}-p{}-mse{}-d{}-a{}.avi\".format(video_output, id_, nb_particle,\\\n",
    "                                                                   sigma_MSE, sigma_d, alpha),\\\n",
    "                              fourcc, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        out.write(frame)\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while(pres_debate.isOpened()):\n",
    "        ret, frame = pres_debate.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        particles, weights, template_yx = update_template(particles, weights, template, gray, sigma_d, sigma_MSE)\n",
    "        \n",
    "        mean_p = np.mean(particles, axis=0)\n",
    "        mean_rect_LT = [int(mean_p[1] - win_w/2), int(mean_p[0] - win_h/2)]\n",
    "        mean_rect_RB = [int(mean_p[1] + win_w/2), int(mean_p[0] + win_h/2)]\n",
    "        \n",
    "        # Update template\n",
    "        best_window = gray[int(template_yx[0] - int(template.shape[0]/2)) : int(template_yx[0] +\\\n",
    "                                                                                int(template.shape[0]/2+0.5)),\\\n",
    "                           int(template_yx[1] - int(template.shape[1]/2)) : int(template_yx[1] +\\\n",
    "                                                                                int(template.shape[1]/2+0.5))]\n",
    "        template = alpha * np.matrix(template, dtype=np.float64) + (1 - alpha) * np.matrix(best_window, dtype=\\\n",
    "                                                                                           np.float64)\n",
    "\n",
    "        for p in particles:\n",
    "            frame = cv2.circle(frame, (int(p[1]), int(p[0])), 1, (255, 0, 0), 1)\n",
    "        frame = cv2.rectangle(frame, (mean_rect_LT[0], mean_rect_LT[1]), (mean_rect_RB[0], mean_rect_RB[1]),\\\n",
    "                              (0, 255, 0), 1)\n",
    "        if all_frames:\n",
    "            cv2.imwrite('./{}/{}.png'.format(id_, count), frame)\n",
    "        if count in frame_output:\n",
    "            cv2.imwrite('ps6-{}-{}-p{}-mse{}-d{}-a{}.png'.format(id_, count, nb_particle, sigma_MSE, sigma_d, alpha),\\\n",
    "                        frame)\n",
    "        count += 1\n",
    "        if video_output:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            out.write(frame)\n",
    "        #break\n",
    "    pres_debate.release()\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Elapsed: \", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed:  10.899605989456177\n"
     ]
    }
   ],
   "source": [
    "particle_filter_appearance_model(nb_particle=50, sigma_d=120, sigma_MSE=10, alpha=0.5, video_output=\\\n",
    "                                 'pres_debate_hand_tracking', origin_temp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ps6-2-1-15-p30-mse10-d120-a0.5.png\" width=\"500\" alt=\"particle30\" align=left />\n",
    "<img src=\"ps6-2-1-50-p30-mse10-d120-a0.5.png\" width=\"500\" alt=\"particle30\" align=left />\n",
    "<img src=\"ps6-2-1-140-p30-mse10-d120-a0.5.png\" width=\"500\" alt=\"particle30\" align=left />\n",
    "<img src=\"ps6-2-1-template.png\" width=\"100\" alt=\"particle30\" align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Try running the tracker on noisy debate.avi. Adjust the parameters until you are able to track the hand all the way to frame 140. Indicate what parameters you had to change to get this to work on the noisy video and discuss why this would be the case.\n",
    "\n",
    "Output: The code, the 3 image frames with overlaid visualizations, the discussion in the pdf, and the image patch used for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed:  19.87918782234192\n"
     ]
    }
   ],
   "source": [
    "particle_filter_appearance_model(video_file='noisy_debate.avi', nb_particle=50, alpha=0.3,\\\n",
    "                                     sigma_d=150, sigma_MSE=10, frame_output=[15, 50, 140],id_='2-2',\\\n",
    "                                     all_frames=False, video_output='noisy_debate_hand_tracking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "I tried to use lower alpha and larger sigma for prediction stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ps6-2-2-15-p50-mse10-d150-a0.3.png\" width=\"500\" alt=\"particle30\" align=left />\n",
    "<img src=\"ps6-2-2-50-p50-mse10-d150-a0.3.png\" width=\"500\" alt=\"particle30\" align=left />\n",
    "<img src=\"ps6-2-2-140-p50-mse10-d150-a0.3.png\" width=\"500\" alt=\"particle30\" align=left />\n",
    "<img src=\"ps6-2-2-template.png\" width=\"100\" alt=\"particle30\" align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Incorporating More Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Run the tracker and save the video frames 40, 100, and 240 with the visualizations overlaid. You will receive partial credit if you can show the tracking size estimate (illustrate this with the rectangle outline) up to frame 100. You will receive full credit if you can reliably track all the way to the end of the street and deal gracefully with the occlusions (reasonable tracking at frame 240).\n",
    "\n",
    "Output: The code, the 3 image frames with overlaid visualizations, and the image patch used for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_template(template, image, mean_p, scaling_factor, sigma_MSE):\n",
    "    win_h, win_w = template.shape\n",
    "    \n",
    "    new_win_h = int(win_h * scaling_factor)\n",
    "    new_win_w = int(win_w * scaling_factor)\n",
    "    \n",
    "    template_reshaped = cv2.resize(template, (new_win_w, new_win_h))\n",
    "    \n",
    "    x = int(mean_p[1])\n",
    "    y = int(mean_p[0])\n",
    "    image_part = image[y - int(new_win_h/2) : y + int(new_win_h/2+0.5), x - int(new_win_w/2) : x + int(new_win_w/2+0.5)]\n",
    "    if template_reshaped.shape != image_part.shape:\n",
    "        return None, 0\n",
    "    diff = np.matrix(template_reshaped, dtype=np.float64) - np.matrix(image_part, dtype=np.float64)\n",
    "    MSE = 1/(new_win_h * new_win_w) * np.sum(np.multiply(diff, diff))\n",
    "    return template_reshaped, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_filter_dynamic_window(video_file='pedestrians.avi', txt_file='pedestrians.txt', nb_particle=10,\\\n",
    "                    sigma_d=30, sigma_MSE=10, frame_output=[40, 100, 240], id_='3-1', alpha=0.5,\\\n",
    "                    all_frames=False, video_output='pedestrians_tracking'):\n",
    "    pres_debate = cv2.VideoCapture(video_file)\n",
    "    count = 0\n",
    "    \n",
    "    rect_x, rect_y, rect_w, rect_h = read_bounding_box(txt_file)\n",
    "    init_particle_center = np.array((int(rect_y + rect_h/2), int(rect_x + rect_w/2)))\n",
    "    \n",
    "    weights = np.ones((nb_particle)) / nb_particle\n",
    "\n",
    "    particles = np.random.multinomial(100, [1/2.] * 2, size=nb_particle)\n",
    "    particles = particles - 50\n",
    "    for idx in range(len(particles)):\n",
    "        particles[idx] = particles[idx] + init_particle_center\n",
    "    ret, frame = pres_debate.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    template = gray[rect_y : rect_y + rect_h, rect_x : rect_x + rect_w]\n",
    "    win_h, win_w = template.shape\n",
    "    \n",
    "    count += 1\n",
    "    for p in particles:\n",
    "        frame = cv2.circle(frame, (int(p[1]), int(p[0])), 1, (255, 0, 0), 1)\n",
    "    frame = cv2.rectangle(frame, (rect_x, rect_y), (rect_x+rect_w, rect_y+rect_h),(0, 255, 0), 1)\n",
    "    if all_frames:\n",
    "        cv2.imwrite('./{}/0.png'.format(id_), frame)\n",
    "    \n",
    "    if video_output:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(\"{}-{}.avi\".format(video_output, id_), fourcc, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        out.write(frame)\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while(pres_debate.isOpened()):\n",
    "        ret, frame = pres_debate.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        particles, weights = update(particles, weights, template, gray, sigma_d, sigma_MSE)\n",
    "    \n",
    "        mean_p = np.mean(particles, axis=0)\n",
    "        \n",
    "        # Update template size\n",
    "        scaling_list = np.arange(0.999, 1.0, 0.0001)\n",
    "        min_MSE = np.inf\n",
    "        for s in scaling_list:\n",
    "            new_template, MSE = scaling_template(template=template, image=gray, mean_p=mean_p,\\\n",
    "                                                  scaling_factor=s, sigma_MSE=sigma_MSE)\n",
    "            if MSE < np.inf:\n",
    "                best_scaling = s\n",
    "        #print(best_scaling)\n",
    "        #scaling = (1 - alpha) * 1 + alpha * best_scaling\n",
    "        template = cv2.resize(template, (0, 0), template, fx=best_scaling, fy=best_scaling)\n",
    "        win_h, win_w = template.shape\n",
    "        \n",
    "        mean_rect_LT = [int(mean_p[1] - win_w/2), int(mean_p[0] - win_h/2)]\n",
    "        mean_rect_RB = [int(mean_p[1] + win_w/2), int(mean_p[0] + win_h/2)]\n",
    "\n",
    "        for p in particles:\n",
    "            frame = cv2.circle(frame, (int(p[1]), int(p[0])), 1, (255, 0, 0), 1)\n",
    "        frame = cv2.rectangle(frame, (mean_rect_LT[0], mean_rect_LT[1]), (mean_rect_RB[0], mean_rect_RB[1]),\\\n",
    "                              (0, 255, 0), 1)\n",
    "        \n",
    "        if all_frames:\n",
    "            cv2.imwrite('./{}/{}.png'.format(id_, count), frame)\n",
    "        if count in frame_output:\n",
    "            cv2.imwrite('ps6-{}-{}.png'.format(id_, count), frame)\n",
    "        count += 1\n",
    "        if video_output:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            out.write(frame)\n",
    "        #break\n",
    "    pres_debate.release()\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Elapsed: \", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed:  6.397747993469238\n"
     ]
    }
   ],
   "source": [
    "particle_filter_dynamic_window(nb_particle=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ps6-3-1-40.png\" width=\"300\" align=left />\n",
    "<img src=\"ps6-3-1-100.png\" width=\"300\" align=left />\n",
    "<img src=\"ps6-3-1-240.png\" width=\"300\" align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Try and optimize the number of particles needed to track the model in this video. Compare that to the number you found in problem 1.4. Why is this number different?\n",
    "\n",
    "Output: The number of particles you found to be optimal and discussion in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed:  18.088554859161377\n"
     ]
    }
   ],
   "source": [
    "particle_filter_dynamic_window(nb_particle=50, id_=\"3-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ps6-3-2-40.png\" width=\"300\" align=left />\n",
    "<img src=\"ps6-3-2-100.png\" width=\"300\" align=left />\n",
    "<img src=\"ps6-3-2-240.png\" width=\"300\" align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "In 3-1, I use 10 particles. Then I tried 30, 50, 100 particles. But did not improve the performance. When there is a man who passed through the screen in front, the tracker could not find the woman anymore, even if I increased the sigma in the prediction stage, which did not improve. Maybe a better way is to introduce a motion model for the woman's previous movement and predict her next step on it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
