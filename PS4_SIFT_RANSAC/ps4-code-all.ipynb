{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_trans_a = cv2.imread('transA.jpg')\n",
    "img_trans_b = cv2.imread('transB.jpg')\n",
    "img_sim_a = cv2.imread('simA.jpg')\n",
    "img_sim_b = cv2.imread('simB.jpg')\n",
    "img_trans_a_gray = cv2.cvtColor(img_trans_a, cv2.COLOR_BGR2GRAY)\n",
    "img_trans_b_gray = cv2.cvtColor(img_trans_b, cv2.COLOR_BGR2GRAY)\n",
    "img_sim_a_gray = cv2.cvtColor(img_sim_a, cv2.COLOR_BGR2GRAY)\n",
    "img_sim_b_gray = cv2.cvtColor(img_sim_b, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_THRESHOLD = 150\n",
    "HARRIS_WIN_SIZE = 11\n",
    "R_RADIUS = 11\n",
    "HIST_BINS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Harris corners\n",
    "\n",
    "In class and in the text we have developed the Harris operator. To find the Harris points you need to compute the gradients in both the X and Y directions. These will probably have to be lightly filtered using a Gaussian to be well behaved. You can do this either the “naive” way - filter the image and then do simple difference between left and right (X gradient) or up and down (Y gradient) - or you can take an analytic derivative of a Gaussian in X or Y and use that filtter. The scale of the filtering is up to you. You may play with the size of the Gaussian as it will interact window size of the corner detection.\n",
    "\n",
    "1.1 Write functions to compute both the X and Y gradients. Try your code on both transA and simA. To display the output, adjoin the two gradient images(X and Y) to make a new, twice as wide, single image (the ”gradient-pair”). Since gradients have negaitve and positiive values, you’ll need to produce and image that is gray for 0.0 and balck is negative and white is positive.\n",
    "\n",
    "Output: The code, and the gradient-pair image for both transA and simA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_x_gradient(img):\n",
    "    gradient = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)\n",
    "    r = np.max(gradient) - np.min(gradient)\n",
    "    gradient = 255 * (gradient - np.min(gradient)) / r\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_y_gradient(img):\n",
    "    gradient = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)\n",
    "    r = np.max(gradient) - np.min(gradient)\n",
    "    gradient = 255 * (gradient - np.min(gradient)) / r\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_trans_a = cv2.GaussianBlur(img_trans_a_gray, (7, 7), 1)\n",
    "trans_a_x = find_x_gradient(filtered_trans_a)\n",
    "trans_a_y = find_y_gradient(filtered_trans_a)\n",
    "trans_a_gradient_pair=cv2.hconcat([trans_a_x,trans_a_y])\n",
    "cv2.imwrite('ps4-1-1-a.png', trans_a_gradient_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sim_a = cv2.GaussianBlur(img_sim_a_gray, (7, 7), 1)\n",
    "sim_a_x = find_x_gradient(filtered_sim_a)\n",
    "sim_a_y = find_y_gradient(filtered_sim_a)\n",
    "sim_a_gradient_pair=cv2.hconcat([sim_a_x,sim_a_y])\n",
    "cv2.imwrite('ps4-1-1-b.png', sim_a_gradient_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-1-1-a](./ps4-1-1-a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-1-1-b](./ps4-1-1-b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Write code to compute the Harris value. You can try the weights just equal to 1. But it might work better with a smoother Gaussian that is higher at the middle and falls off gradually. Your output is a scalar function. Apply to transA, transB, simA, and simB.\n",
    "\n",
    "Output: The code and the Harris value output image for each of the images. To display the output reasonably you will have to scale the image values to be in a range of 0-255 or 0.0 to 1.0, depending upon how you deal with images.\n",
    "Finally you can find some corner points. To do this requires two steps: thresholding and non- maximal suppression. You’ll need to choose a threshold value that eliminates points that don’t seem to be plausible corners. And for the non-maximal suppression, you’ll need to choose a radius (could be size of a side of a square window instead of a circle radius) over which a pixel has to be a maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Harris(img, win_size):\n",
    "    height, width = img.shape\n",
    "    R = np.zeros((height - win_size + 1, width - win_size + 1))\n",
    "    x_gradient = find_x_gradient(img)/255.0\n",
    "    y_gradient = find_y_gradient(img)/255.0\n",
    "    alpha = 0.05\n",
    "    for ix in range(width - win_size + 1):\n",
    "        for iy in range(height - win_size + 1):\n",
    "            Ixx = x_gradient[iy:iy+win_size, ix:ix+win_size] ** 2 / (win_size ** 2)\n",
    "            Ixx = Ixx.sum()\n",
    "            Iyy = y_gradient[iy:iy+win_size, ix:ix+win_size] ** 2 / (win_size ** 2)\n",
    "            Iyy = Iyy.sum()\n",
    "            Ixy = np.dot(x_gradient[iy:iy+win_size, ix:ix+win_size], y_gradient[iy:iy+win_size, ix:ix+win_size]) / (win_size ** 2)\n",
    "            Ixy = Ixy.sum()\n",
    "            \n",
    "            M = [[Ixx, Ixy], [Ixy, Iyy]]\n",
    "            R[iy, ix] = np.linalg.det(M) - alpha * (np.trace(M) ** 2)\n",
    "    R_range = np.max(R) - np.min(R)\n",
    "    R_pic = 255 * (R - np.min(R)) / R_range\n",
    "    R_pic = abs(128 - R_pic) * 2\n",
    "    return R_pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_Harris(R, t, r):\n",
    "    R_out = R.copy()\n",
    "    R_out[np.where(R<t)] = 0\n",
    "\n",
    "    height, width = R.shape\n",
    "    for dx in range(width//r):\n",
    "        for dy in range(height//r):\n",
    "            ix = dx * r\n",
    "            iy = dy * r\n",
    "            w = R_out[iy:iy+r, ix:ix+r]\n",
    "            local_max = np.max(w)\n",
    "            arg_max = np.argmax(w)\n",
    "            y_max = arg_max // r\n",
    "            x_max = arg_max % r\n",
    "            R_out[iy:iy+r, ix:ix+r] = 0\n",
    "            R_out[iy + y_max, ix + x_max] = local_max\n",
    "    return R_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_trans_a = compute_Harris(img_trans_a_gray, HARRIS_WIN_SIZE)\n",
    "R_trans_a_t = threshold_Harris(R_trans_a, R_THRESHOLD, R_RADIUS)\n",
    "trans_a_R_pair=cv2.hconcat([R_trans_a,R_trans_a_t])\n",
    "cv2.imwrite('ps4-1-2-a.png', trans_a_R_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_trans_b = compute_Harris(img_trans_b_gray, HARRIS_WIN_SIZE)\n",
    "R_trans_b_t = threshold_Harris(R_trans_b, R_THRESHOLD, R_RADIUS)\n",
    "trans_b_R_pair=cv2.hconcat([R_trans_b,R_trans_b_t])\n",
    "cv2.imwrite('ps4-1-2-b.png', trans_b_R_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_sim_a = compute_Harris(img_sim_a_gray, HARRIS_WIN_SIZE)\n",
    "R_sim_a_t = threshold_Harris(R_sim_a, R_THRESHOLD, R_RADIUS)\n",
    "sim_a_R_pair=cv2.hconcat([R_sim_a,R_sim_a_t])\n",
    "cv2.imwrite('ps4-1-2-c.png', sim_a_R_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_sim_b = compute_Harris(img_sim_b_gray, HARRIS_WIN_SIZE)\n",
    "R_sim_b_t = threshold_Harris(R_sim_b, R_THRESHOLD, R_RADIUS)\n",
    "sim_b_R_pair=cv2.hconcat([R_sim_b,R_sim_b_t])\n",
    "cv2.imwrite('ps4-1-2-d.png', sim_b_R_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-1-2-a](./ps4-1-2-a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-1-2-b](./ps4-1-2-b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-1-2-c](./ps4-1-2-c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-1-2-d](./ps4-1-2-d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Write a function to threshold and do non-maximal suppression on the Harris output. Surprise, huh? Adjust the threshold and radius until you get a “nice” set of points, probably on the order of a hundred or two (or three?). But use your judgment in terms of getting enough points. Are there any points that are not found in both images?\n",
    "\n",
    "Output: The code. Apply your function to both image pair: (transA, transB) and (simA, simB). Mark the corners visibly in each of the four result images and provide those images. Also, describe the behavior of your corner detector including anything surprising, such as points not found in both images of a pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_Harris(img, r, t, w):\n",
    "    img_out = img.copy()\n",
    "    for ix in range(r.shape[1] - 11):\n",
    "        for iy in range(r.shape[0] - 11):\n",
    "            if r[iy, ix] > t and r[iy, ix] != 255:\n",
    "                img_out = cv2.circle(img_out, (ix+w, iy+w), 5, (0,255,0), 1)\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_a_harris = draw_Harris(img_trans_a, R_trans_a_t, 128, 5)\n",
    "cv2.imwrite('ps4-1-3-a.png', trans_a_harris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_b_harris = draw_Harris(img_trans_b, R_trans_b_t, 128, 5)\n",
    "cv2.imwrite('ps4-1-3-b.png', trans_b_harris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_a_harris = draw_Harris(img_sim_a, R_sim_a_t, 128, 5)\n",
    "cv2.imwrite('ps4-1-3-c.png', sim_a_harris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_b_harris = draw_Harris(img_sim_b, R_sim_b_t, 128, 5)\n",
    "cv2.imwrite('ps4-1-3-d.png', sim_b_harris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-1-3-a](./ps4-1-3-a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-1-3-b](./ps4-1-3-b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-1-3-c](./ps4-1-3-c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-1-3-d](./ps4-1-3-d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The R returns an array of negative numbers and 0; after projecting the values from 0 to 255, I found the flat areas are gray and corners are black or white. So I subtracted the matrix with 128 and remapped the values to 0 to 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 SIFT features\n",
    "\n",
    "Now that you have keypoints for both image pairs, we can compute descriptors. You will be glad to know that we do not expect you to write your own SIFT descriptor code. Instead you’ll use either a MATLAB package called VLFeat or the SIFT or SURF classes in OpenCV. See the accompanying supplemental document for details.\n",
    "The standard use of a SIFT library consists of you just providing an image and the library does its thing: finds interest points at various scales and computes descriptors at each point. We’re going to use the code just to compute the orientation histogram descriptors for the interest points you have already detected from Problem 1. To do so, you need to provide a scale setting and an orientation for each feature point as well as the gradient magnitude and angle for each pixel. The scale we’ll fix to 1.0 (see accompanying SIFT software usage tutorial). The orientation needs to be computed from the gradients:\n",
    "angle = atan2(Iy,Ix)\n",
    "But, you already have the gradient images! So you can create an “angle” image and then for a\n",
    "given feature point at < ui, vi > you can get the gradient direction.\n",
    "\n",
    "\n",
    "2.1 Write the function to compute the angle. Then for the set of interest points you found above, plot the points for all of transA, transB, simA and simB on the respective images and draw a little line that shows the direction of the gradient. In MATLAB, if you want you can use the VLFeat function vl plotframe to draw the feature points locations and the angle. You’ll need to figure it out - look at http://www.vlfeat.org/overview/sift.html and also the documentation for vl plotframe. In OpenCV you can use the method drawKeypoints().\n",
    "\n",
    "Output: The code. And both of the drawn on pairs (transA, transB) and (simA, simB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_orientation(img):\n",
    "    Ix = find_x_gradient(img)\n",
    "    Iy = find_y_gradient(img)\n",
    "    angle = np.arctan2(Iy, Ix) * 360\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_keypoints(r, angle): \n",
    "    keypoints = []\n",
    "    h, w = r.shape\n",
    "    r_normalized = r.copy()\n",
    "    cv2.normalize(r,  r_normalized, 0, 25, cv2.NORM_MINMAX)\n",
    "    for ix in range(w):\n",
    "        for iy in range(h):\n",
    "            if r[iy, ix] > 0:\n",
    "                a = angle[iy + 5,ix + 5]\n",
    "                keypoint = cv2.KeyPoint(x=ix+5, y=iy+5 ,_size=r_normalized[iy, ix],_angle = a)\n",
    "                keypoints.append(keypoint)\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_a_angle = compute_orientation(img_trans_a_gray)\n",
    "trans_a_kp = compute_keypoints(R_trans_a_t, trans_a_angle)\n",
    "\n",
    "trans_b_angle = compute_orientation(img_trans_b_gray)\n",
    "trans_b_kp = compute_keypoints(R_trans_b_t, trans_b_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_a_kp_img = cv2.drawKeypoints(img_trans_a_gray, trans_a_kp, img_trans_a, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "trans_b_kp_img = cv2.drawKeypoints(img_trans_b_gray, trans_b_kp, img_trans_b, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "trans_kp_img_pair=cv2.hconcat([trans_a_kp_img, trans_b_kp_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate OpenCV SIFT keypoints\n",
    "trans_a_kp_t = sift.detect(img_trans_a_gray, None)\n",
    "trans_b_kp_t = sift.detect(img_trans_b_gray, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_a_kp_img_t = cv2.drawKeypoints(img_trans_a_gray, trans_a_kp_t, img_trans_a.copy(), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "trans_b_kp_img_t = cv2.drawKeypoints(img_trans_b_gray, trans_b_kp_t, img_trans_b.copy(), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "trans_kp_img_pair_t = cv2.hconcat([trans_a_kp_img_t, trans_b_kp_img_t])\n",
    "trans_a_kp_compare = cv2.vconcat([trans_kp_img_pair, trans_kp_img_pair_t])\n",
    "cv2.imwrite('ps4-2-1-a.png', trans_a_kp_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 images line 1: keypoints by implemented functions\n",
    "\n",
    "2 images line 2: keypoints by OpenCV SIFT functions\n",
    "![ps4-2-1-a](./ps4-2-1-a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_a_angle = compute_orientation(img_sim_a_gray)\n",
    "sim_a_kp = compute_keypoints(R_sim_a_t, sim_a_angle)\n",
    "\n",
    "sim_b_angle = compute_orientation(img_sim_b_gray)\n",
    "sim_b_kp = compute_keypoints(R_sim_b_t, sim_b_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_a_kp_img = cv2.drawKeypoints(img_sim_a_gray, sim_a_kp, img_sim_a, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "sim_b_kp_img = cv2.drawKeypoints(img_sim_b_gray, sim_b_kp, img_sim_b, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "sim_kp_img_pair=cv2.hconcat([sim_a_kp_img, sim_b_kp_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate OpenCV SIFT keypoints & descriptors\n",
    "sim_a_kp_t = sift.detect(img_sim_a_gray, None)\n",
    "sim_b_kp_t = sift.detect(img_sim_b_gray, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_a_kp_img_t = cv2.drawKeypoints(img_sim_a_gray, trans_a_kp_t, img_sim_a, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "sim_b_kp_img_t = cv2.drawKeypoints(img_sim_b_gray, sim_b_kp_t, img_sim_b, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "sim_kp_img_pair_t = cv2.hconcat([sim_a_kp_img_t, sim_b_kp_img_t])\n",
    "sim_a_kp_compare = cv2.vconcat([sim_kp_img_pair, sim_kp_img_pair_t])\n",
    "cv2.imwrite('ps4-2-1-b.png', sim_a_kp_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 images line 1: keypoints by implemented functions\n",
    "\n",
    "2 images line 2: keypoints by OpenCV SIFT functions\n",
    "![ps4-2-1-b](./ps4-2-1-b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Write the function to call the appropriate SIFT descriptor extraction function with the necessary input data structures. Do this for all the keypoints in both pairs of images. Then call the matching functions of VLFeat or OpenCV to compute the best matches between the left and right images of each pair. Then create the putative-pair-image for both transA–transB and simA–simB pair. You must write your own drawing function (note you may use OpenCV’s line() function or MATLAB’s plot() function).\n",
    "\n",
    "Output: The code. And both of the putative-pair-images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_descriptor(img, kp):\n",
    "    kp, des = sift.compute(img, kp)\n",
    "    return des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_matches(imgl, kpl, imgr, kpr, matches):\n",
    "    output = cv2.hconcat([imgl, imgr])\n",
    "    height, width, color = imgl.shape\n",
    "    for match in matches:\n",
    "        xl, yl = kpl[match.queryIdx].pt\n",
    "        xr, yr = kpr[match.trainIdx].pt\n",
    "        output = cv2.line(output, (int(xl), int(yl)), (int(xr + width), int(yr)), (0,255,0), 1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_a_dp = call_descriptor(img_trans_a_gray, trans_a_kp)\n",
    "trans_b_dp = call_descriptor(img_trans_b_gray, trans_b_kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher(crossCheck=True)\n",
    "trans_matches = bf.match(trans_a_dp, trans_b_dp)\n",
    "trans_matches = sorted(trans_matches, key = lambda x:x.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_trans_a = cv2.imread('transA.jpg')\n",
    "img_trans_b = cv2.imread('transB.jpg')\n",
    "img_sim_a = cv2.imread('simA.jpg')\n",
    "img_sim_b = cv2.imread('simB.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_matching_pair = draw_matches(img_trans_a, trans_a_kp, img_trans_b, trans_b_kp, trans_matches[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('ps4-2-2-a.png', trans_matching_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-2-2-a](./ps4-2-2-a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_a_dp = call_descriptor(img_sim_a_gray, sim_a_kp)\n",
    "sim_b_dp = call_descriptor(img_sim_b_gray, sim_b_kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher(crossCheck=True)\n",
    "sim_matches = bf.match(sim_a_dp, sim_b_dp)\n",
    "sim_matches = sorted(sim_matches, key = lambda x:x.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matching_pair = draw_matches(img_trans_a, sim_a_kp, img_trans_b, sim_b_kp, sim_matches[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('ps4-2-2-b.png', sim_matching_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-2-2-b](./ps4-2-2-b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 RANSAC\n",
    "\n",
    "We’re almost there. You now have keypoints, descriptors and their putative matches. What remains is RANSAC. To do this for the translation case is easy. Using the matched keypoints for transA and transB, randomly select one of the putative matches. This will give you an offset (a translation in X and Y ) between the two images. Find out how many other putative matches agree with this offset (remember, you may have to account for noise, so ”agreeing” means within some tolerance). This is the consensus set for the selected first match. Find the best such translation - the one with the biggest consensus set.\n",
    "\n",
    "3.1 Write the code to do the translational case on transA and transB. Draw the lines on the adjoined images of the biggest consensus set.\n",
    "\n",
    "Output: The code and the drawn upon adjoined image pairs. \n",
    "\n",
    "Also, say what the translation vector was and what percentages of your matches was the biggest consensus set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_ransac(imgl, imgr, kpl, kpr, matches):\n",
    "    outimg = cv2.hconcat([imgl, imgr])\n",
    "    height, width, color = imgl.shape\n",
    "    \n",
    "    n = len(kpl)\n",
    "    max_matching_count = 0\n",
    "    max_matching_transx = 0\n",
    "    max_matching_transy = 0\n",
    "    max_matching_list = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        rand_matching = np.random.choice(matches)\n",
    "        rand_xl, rand_yl = kpl[rand_matching.queryIdx].pt\n",
    "        rand_xr, rand_yr = kpr[rand_matching.trainIdx].pt\n",
    "        trans_x = rand_xr - rand_xl\n",
    "        trans_y = rand_xr - rand_yr\n",
    "        matching_list = []\n",
    "        for match in matches:\n",
    "            xl, yl = kpl[match.queryIdx].pt\n",
    "            xr, yr = kpr[match.trainIdx].pt\n",
    "            if xl + trans_x - xr < 5 and yl + trans_y - yr < 5:\n",
    "                matching_list.append(match)\n",
    "        if len(matching_list) > max_matching_count:\n",
    "            max_matching_count = len(matching_list)\n",
    "            max_transx = trans_x\n",
    "            max_transy = trans_y\n",
    "            max_matching_list = matching_list\n",
    "    print(\"Best matching points:\", max_matching_count, \"\\nKeypoints in left img:\", len(kpl),\\\n",
    "          \"\\nKeypoints in right img:\", len(kpr), \"\\nBest matches:\", len(matches))\n",
    "    for match in max_matching_list:\n",
    "        xl, yl = kpl[match.queryIdx].pt\n",
    "        xr, yr = kpr[match.trainIdx].pt\n",
    "        outimg = cv2.line(outimg, (int(xl), int(yl)), (int(xr + width), int(yr)), (0,255,0), 1)\n",
    "          \n",
    "    return outimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matching points: 48 \n",
      "Keypoints in left img: 503 \n",
      "Keypoints in right img: 795 \n",
      "Best matches: 100\n"
     ]
    }
   ],
   "source": [
    "trans_ransac = translation_ransac(img_trans_a, img_trans_b, trans_a_kp, trans_b_kp, trans_matches[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('ps4-3-1.png', trans_ransac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The translation is two vectors in direction x and y, so I didn't use a transition matrix but get the vectors only by subtracting the x and y variables in two images. When finding the corresponding point, I took the point in the left image and added by the translation vector x and y, compare the result with the point in the right image.\n",
    "\n",
    "I set 5 as the tolerance of the difference, and got 48 best matching points out of 100 pairs of matching point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-3-1](./ps4-3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Do the same as above but for the similarity pair simA and simB. Write code to apply RANSAC by randomly picking two matches, solving for the transform, and determining the consensus set. Draw the lines on the adjoined images for the biggest consensus set.\n",
    "\n",
    "\n",
    "Output: The code and the drawn upon adjoined image pairs. \n",
    "\n",
    "Also, say what the transform matrix is for best set and what percentages of your matches was the biggest consensus set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_ransac(imgl, imgr, kpl, kpr, matches):\n",
    "    outimg = cv2.hconcat([imgl, imgr])\n",
    "    height, width, color = imgl.shape\n",
    "    \n",
    "    n = len(kpl)\n",
    "    max_matching_count = 0\n",
    "    max_matching_matrix = []\n",
    "    max_matching_list = []\n",
    "    for i in range(int(0.95*n)):\n",
    "        rand_matching1, rand_matching2 = np.random.choice(matches, size=2)\n",
    "        rand_xl1, rand_yl1 = kpl[rand_matching1.queryIdx].pt\n",
    "        rand_xr1, rand_yr1 = kpr[rand_matching1.trainIdx].pt\n",
    "        rand_xl2, rand_yl2 = kpl[rand_matching2.queryIdx].pt\n",
    "        rand_xr2, rand_yr2 = kpr[rand_matching2.trainIdx].pt\n",
    "        A = np.array([[rand_xl1, rand_yl1, 1, 0], [rand_yl1, -rand_xl1, 0, 1],\\\n",
    "                      [rand_xl2, rand_yl2, 1, 0], [rand_yl2, -rand_xl2, 0, 1]])\n",
    "        B = np.array([rand_xr1, rand_yr1, rand_xr2, rand_yr2])\n",
    "        s = np.linalg.pinv(A) * B\n",
    "        s = s[0]\n",
    "        matching_list = []\n",
    "        for match in matches:\n",
    "            xl, yl = kpl[match.queryIdx].pt\n",
    "            xr, yr = kpr[match.trainIdx].pt\n",
    "            tA = np.array([[xl, yl, 1, 0], [yl, -xl, 0, 1]])\n",
    "            tB = np.array([xr, yr])\n",
    "            sim = np.matmul(tA, s)\n",
    "            if np.sum(sim - tB) < 5:\n",
    "                matching_list.append(match)\n",
    "        if len(matching_list) > max_matching_count:\n",
    "            max_matching_count = len(matching_list)\n",
    "            max_matching_matrix = s\n",
    "            max_matching_list = matching_list\n",
    "    print(\"Best matching points:\", max_matching_count, \"\\nKeypoints in left img:\", len(kpl),\\\n",
    "          \"\\nKeypoints in right img:\", len(kpr), \"\\nBest matches:\", len(matches))\n",
    "    for match in max_matching_list:\n",
    "        xl, yl = kpl[match.queryIdx].pt\n",
    "        xr, yr = kpr[match.trainIdx].pt\n",
    "        outimg = cv2.line(outimg, (int(xl), int(yl)), (int(xr + width), int(yr)), (0,255,0), 1)\n",
    "          \n",
    "    return outimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matching points: 87 \n",
      "Keypoints in left img: 398 \n",
      "Keypoints in right img: 263 \n",
      "Best matches: 87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_ransac = similarity_ransac(img_sim_a, img_sim_b, sim_a_kp, sim_b_kp, sim_matches)\n",
    "cv2.imwrite('ps4-3-2.png', sim_ransac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ps4-3-2](./ps4-3-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "For similarity, we only need two pairs of points to make a transition matrix. After calculating, we have:\n",
    "\n",
    "A = [[x y 1 0]  [y -x 0 1]]\n",
    "\n",
    "B = [x' y']\n",
    "\n",
    "A*s = B\n",
    "\n",
    "By using pinv we obtain s and apply s on all matching points we can have inliers. With a tolerance of 5, 100% percent of matchings are inliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
